{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Group\n",
    "## Lab Three: Extending Logistic Regression\n",
    "### Wali Chaudhary, Bryce Shurts, & Alex Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview\n",
    "\n",
    "### (Business) Use-case\n",
    "\n",
    "As is outlined by Mr. Nansal's dataset description, a salient goal of this dataset was to provide an accurate/representative dataset regarding the failures of machines and other mechanical equipment, as real datasets of such a nature (and of a sufficient quality so as to be predictive) are difficult to find in the wild.\n",
    "\n",
    "Naturally, this flows into the overall goal of having such a predictive dataset: increasing our ability to monitor potentially dangerous equipment for wear (or other failure conditions) and enact preventative maintenance or usage guidelines for them to prevent this from happening. While in some cases this is more of a financial and business continuity concern, e.g., the operation of an assembly line creating clothing, it can quickly spiral into health and safety concern, both for employees and others that are dependent on such tools doing their job accurately and safely: automated forge equipment, QA testing machines for foods and chemical products and tools used to create load-bearing equipment are just some of these possibilities.\n",
    "\n",
    "As such, it is of great importance to a business financially, morally, and legally to ensure that they have reliable methods for detecting potential and upcoming failures of equipment, which this dataset could potentially help with should an accurate & generalizable classifier be created around it.\n",
    "\n",
    "#### Citation & Acknowledgement\n",
    "\n",
    "The *Machine Predictive Maintenance Classification* dataset is liscenced under the [Creative Commons 1.0 Universal Public Domain](https://creativecommons.org/publicdomain/zero/1.0/) and was provided by Shivam Nansal on [Kaggle](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification).\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Before we begin preprocessing, we must first break down the contents of our data and describe the nature and repersentation of each feature:\n",
    "\n",
    "- UDI - unique ID - categorical; one-hot encoded integer\n",
    "- Product ID - Another unique identifier - categorical; one-hot encoded integer\n",
    "- Type - Specifies the quality variant of the product (low, medium, high) - categorical; one-hot encoded integer\n",
    "- Air temperature [K] - Ambient temp in K at time of sampling - numerical; float\n",
    "- Process temperature [K] - Synthetic estimate of tool temperature in K generated via stochastic method - numerical; float\n",
    "- Rotational speed [rpm] - Estimated RPM of tool at time of sampling - numerical; integer\n",
    "- Torque [Nm] - Output torque of tool in Nm at time of sampling - numerical; float\n",
    "- Tool wear [min] - Amount of time the tool has been used for in minutes - numerical; integer\n",
    "- Target - Binary of whether a failure occured - categorical; binary integer/boolean\n",
    "- Failure Type - Mode of failure, if any: classification target - categorical; one-hot encoded integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle all imports for notebook\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import (DataFrame, Series)\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from numpy.linalg import pinv\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "# Comparison imports for Dr. Larson's code\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from numpy import ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = pd.read_csv(\"predictive_maintenance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0    1     M14860    M                298.1                    308.6   \n",
       "1    2     L47181    L                298.2                    308.7   \n",
       "2    3     L47182    L                298.1                    308.5   \n",
       "3    4     L47183    L                298.2                    308.6   \n",
       "4    5     L47184    L                298.2                    308.7   \n",
       "\n",
       "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
       "0                    1551         42.8                0       0   No Failure  \n",
       "1                    1408         46.3                3       0   No Failure  \n",
       "2                    1498         49.4                5       0   No Failure  \n",
       "3                    1433         39.5                7       0   No Failure  \n",
       "4                    1408         40.0                9       0   No Failure  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, we can determine that `UDI` and `Product ID` will not be useful to us, as we do not care about features that are redundant in function to the given row ID (i.e., features that are unique to each sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0    M                298.1                    308.6                    1551   \n",
       "1    L                298.2                    308.7                    1408   \n",
       "2    L                298.1                    308.5                    1498   \n",
       "3    L                298.2                    308.6                    1433   \n",
       "4    L                298.2                    308.7                    1408   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
       "0         42.8                0       0   No Failure  \n",
       "1         46.3                3       0   No Failure  \n",
       "2         49.4                5       0   No Failure  \n",
       "3         39.5                7       0   No Failure  \n",
       "4         40.0                9       0   No Failure  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"UDI\", \"Product ID\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove the `Target` feature, as we are looking to solve a multiclass classification problem and leaving this feature in will simply result in leakage where the classifier does not learn the expected relationship between the other features and `Failure Type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Failure Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0    M                298.1                    308.6                    1551   \n",
       "1    L                298.2                    308.7                    1408   \n",
       "2    L                298.1                    308.5                    1498   \n",
       "3    L                298.2                    308.6                    1433   \n",
       "4    L                298.2                    308.7                    1408   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min] Failure Type  \n",
       "0         42.8                0   No Failure  \n",
       "1         46.3                3   No Failure  \n",
       "2         49.4                5   No Failure  \n",
       "3         39.5                7   No Failure  \n",
       "4         40.0                9   No Failure  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"Target\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can check for any invalid data, such as missing/null entries and non-sensical numbers (negative time or torque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries: \n",
      "Type                       0\n",
      "Air temperature [K]        0\n",
      "Process temperature [K]    0\n",
      "Rotational speed [rpm]     0\n",
      "Torque [Nm]                0\n",
      "Tool wear [min]            0\n",
      "Failure Type               0\n",
      "dtype: int64\n",
      "\n",
      "Invalid values: \n",
      "Torque: False\n",
      "Tool wear: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing entries: \\n\" + str(df.isna().sum()))\n",
    "\n",
    "print(\"\\nInvalid values: \\nTorque: \" + str((df[\"Torque [Nm]\"].values < 0).any())\n",
    "      + \"\\nTool wear: \" + str((df[\"Tool wear [min]\"].values < 0).any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have checked the data and ensured its validity, we can move on to our dummy encoding of our `Type` feature & label encoding our `Failure Type` target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Failure Type</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
       "0                298.1                    308.6                    1551   \n",
       "1                298.2                    308.7                    1408   \n",
       "2                298.1                    308.5                    1498   \n",
       "3                298.2                    308.6                    1433   \n",
       "4                298.2                    308.7                    1408   \n",
       "\n",
       "   Torque [Nm]  Tool wear [min]  Failure Type  H  L  M  \n",
       "0         42.8                0             1  0  0  1  \n",
       "1         46.3                3             1  0  1  0  \n",
       "2         49.4                5             1  0  1  0  \n",
       "3         39.5                7             1  0  1  0  \n",
       "4         40.0                9             1  0  1  0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df[\"Type\"])], axis=1)\n",
    "df.drop(\"Type\", axis=1, inplace=True)\n",
    "df[\"Failure Type\"] = (df[\"Failure Type\"].astype(\"category\")).cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization / Standardization goes here! If required, otherwise delete this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/Training Split\n",
    "Finally, we are presented with the option of splitting our data into a training & test split. In our case we believe this is the appropiate action to take, as we can train our agent on around 8,000 samples of our data & then cross-validate that it has learned the ground truth relationship between features & the target by attempting to inference the agents against the test data. If the accuracy drops significantly when testing against this test data, then we will know that our model has overfit the data (or otherwise failed to establish the relationship between the feature vectors & the classification vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m gabr_feature\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# # KNN Classification\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m gabor_res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ compute_gabor(row, kernels, original_shape) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m images_2 ])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# knn_gab = KNeighborsClassifier(n_neighbors=1)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# gab_train, gab_test, y_train, y_test = train_test_split(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pairwise Distances & Heatmap\u001b[39;00m\n\u001b[1;32m     44\u001b[0m gabor_pairwise \u001b[38;5;241m=\u001b[39m pairwise_distances(gabor_res)\n",
      "Cell \u001b[0;32mIn[101], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m gabr_feature\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# # KNN Classification\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m gabor_res \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ \u001b[43mcompute_gabor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m images_2 ])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# knn_gab = KNeighborsClassifier(n_neighbors=1)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# gab_train, gab_test, y_train, y_test = train_test_split(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pairwise Distances & Heatmap\u001b[39;00m\n\u001b[1;32m     44\u001b[0m gabor_pairwise \u001b[38;5;241m=\u001b[39m pairwise_distances(gabor_res)\n",
      "Cell \u001b[0;32mIn[101], line 23\u001b[0m, in \u001b[0;36mcompute_gabor\u001b[0;34m(row, kernels, shape)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, kernel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kernels):\n\u001b[1;32m     22\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m ndi\u001b[38;5;241m.\u001b[39mconvolve(row\u001b[38;5;241m.\u001b[39mreshape(shape), kernel, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrap\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     _,_,feats[k,\u001b[38;5;241m0\u001b[39m],feats[k,\u001b[38;5;241m1\u001b[39m],feats[k,\u001b[38;5;241m2\u001b[39m],feats[k,\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# mean, var, skew, kurt    \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feats\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:1522\u001b[0m, in \u001b[0;36mdescribe\u001b[0;34m(a, axis, ddof, bias, nan_policy)\u001b[0m\n\u001b[1;32m   1520\u001b[0m m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(a, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1521\u001b[0m v \u001b[38;5;241m=\u001b[39m _var(a, axis\u001b[38;5;241m=\u001b[39maxis, ddof\u001b[38;5;241m=\u001b[39mddof)\n\u001b[0;32m-> 1522\u001b[0m sk \u001b[38;5;241m=\u001b[39m \u001b[43mskew\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1523\u001b[0m kurt \u001b[38;5;241m=\u001b[39m kurtosis(a, axis, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DescribeResult(n, mm, m, v, sk, kurt)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:477\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    475\u001b[0m contains_nans \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m--> 477\u001b[0m     contains_nan, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_contains_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     contains_nans\u001b[38;5;241m.\u001b[39mappend(contains_nan)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Addresses nan_policy == \"propagate\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Consider adding option to let function propagate nans, but\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# currently the hypothesis tests this is applied to do not\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# propagate nans in a sensible way\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/_lib/_util.py:626\u001b[0m, in \u001b[0;36m_contains_nan\u001b[0;34m(a, nan_policy, use_summation)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_policy \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m policies:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan_policy must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    624\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m policies))\n\u001b[0;32m--> 626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missubdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minexact\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# The summation method avoids creating a (potentially huge) array.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_summation:\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/school/grad/spring 2023/venv/lib/python3.9/site-packages/numpy/core/numerictypes.py:415\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missubdtype\u001b[39m(arg1, arg2):\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    Returns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43missubclass_\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneric\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    416\u001b[0m         arg1 \u001b[38;5;241m=\u001b[39m dtype(arg1)\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg2, generic):\n",
      "File \u001b[0;32m~/Desktop/school/grad/spring 2023/venv/lib/python3.9/site-packages/numpy/core/numerictypes.py:321\u001b[0m, in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arg1, arg2)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gabor Calculation\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "original_shape = (1,8)\n",
    "rand_idx = 3\n",
    "acc_logistic = None\n",
    "images_2 = df.drop(\"Failure Type\", axis=1).to_numpy()\n",
    "y = df[\"Failure Type\"].to_numpy()\n",
    "\n",
    "kernels = []\n",
    "sigma_vals = (1, 3, 5)\n",
    "frequencies = (0.05, 0.15, 0.25, 0.35)\n",
    "for theta in range(num_orientations := 8):\n",
    "    theta = theta / 8. * np.pi    \n",
    "    for sigma in sigma_vals:\n",
    "        for frequency in frequencies: \n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)\n",
    "def compute_gabor(row, kernels, shape):\n",
    "    feats = np.zeros((len(kernels), 4), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(row.reshape(shape), kernel, mode='wrap')\n",
    "        _,_,feats[k,0],feats[k,1],feats[k,2],feats[k,3] = stats.describe(filtered.reshape(-1))\n",
    "        # mean, var, skew, kurt    \n",
    "    return feats.reshape(-1)\n",
    "# gabr_feature = compute_gabor(images_2[rand_idx], kernels, original_shape)\n",
    "gabr_feature = compute_gabor(images_2[rand_idx], kernels, original_shape)\n",
    "gabr_feature.shape\n",
    "\n",
    "# # KNN Classification\n",
    "gabor_res = np.array([ compute_gabor(row, kernels, original_shape) for row in images_2 ])\n",
    "# knn_gab = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# gab_train, gab_test, y_train, y_test = train_test_split(\n",
    "#     gabor_res,y,test_size=0.2, train_size=0.8)\n",
    "# knn_gab.fit(gab_train,y_train)\n",
    "\n",
    "# acc_gab = accuracy_score(knn_gab.predict(gab_test),y_test)\n",
    "# print(f\"Gabor shape: {gabor_res.shape}\")\n",
    "# print(f\"Logistic Classifier accuracy: {100*acc_logistic:.2f}%\")\n",
    "# print(f\"Gabor accuracy: {100*acc_gab:.2f}%\")\n",
    "\n",
    "# Pairwise Distances & Heatmap\n",
    "gabor_pairwise = pairwise_distances(gabor_res)\n",
    "scaler = StandardScaler()\n",
    "scaled_pairwise_gabor_imgs = scaler.fit_transform(X=gabor_res, y=None)\n",
    "print(scaled_pairwise_gabor_imgs.shape)\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(scaled_pairwise_gabor_imgs[:155, :155], linewidth=.5, cmap=\"crest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Verus-All Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binary_logistic_classifier:\n",
    "    def __init__(self, learn_rate: float, iterations: int=20, constant: float=0.001, penalty: str=None) -> None:\n",
    "        self.learn_rate = learn_rate\n",
    "        self.iterations = iterations\n",
    "        self.constant = constant\n",
    "        self.penalty = penalty\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(theta: ndarray) -> ndarray:\n",
    "        # ∀x∈θ max(0, x)\n",
    "        return np.maximum(0, theta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(data: ndarray) -> ndarray:\n",
    "        return np.hstack((np.ones((data.shape[0], 1)), data))\n",
    "    \n",
    "    def _penalty(self, gradient: ndarray) -> ndarray:\n",
    "        if self.penalty == None:\n",
    "            return gradient\n",
    "        # LASSO\n",
    "        elif self.penalty == \"L1\":\n",
    "            return gradient + self.constant * np.sum(np.abs(self.w_))\n",
    "        # RIDGE\n",
    "        elif self.penalty == \"L2\":\n",
    "            return gradient + self.constant * np.sum(self.w_**2)\n",
    "        # Elastic Net\n",
    "        elif self.penalty == \"both\":\n",
    "            return gradient + self.constant * np.sum(self.w_**2) + self.constant * np.sum(np.abs(self.w_))\n",
    "        else:\n",
    "            raise Exception(\"'\" + str(self.penalty) +\"' not understood.\")\n",
    "    \n",
    "    \n",
    "    def predict_probability(self, data: ndarray, add_bias: bool=True) -> float:\n",
    "        return self._relu((self._add_bias(data) if add_bias else data) @ self.w_)\n",
    "        \n",
    "    def predict(self, X: ndarray):\n",
    "        return self.predict_probability(X) > 0.5\n",
    "    \n",
    "    def fit(self, data: ndarray, target: ndarray) -> None:\n",
    "        data_bias = self._add_bias(data)\n",
    "        num_samples = data_bias.shape[0] # Rows\n",
    "        num_features = data_bias.shape[1] # Columns\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            gradient = self._get_gradient(data_bias, target)\n",
    "            gradient = self._penalty(gradient)\n",
    "            self.w_ -= self.learn_rate * gradient\n",
    "            \n",
    "class multiclass_logistic_classifier:\n",
    "    def __init__(self, solver: type[binary_logistic_classifier], learn_rate: float, iterations: int=20, constant: float=0.001, penalty: str=None) -> None:\n",
    "        self.solver = solver\n",
    "        self.learn_rate = learn_rate\n",
    "        self.iterations = iterations\n",
    "        self.constant = constant\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    def predict_probabilities(self, X: ndarray) -> float:\n",
    "        probabilities = []\n",
    "        for classifier in self.fit_classifiers:\n",
    "            probabilities.append(classifier.predict_probability(X).reshape((len(X), 1)))\n",
    "        return np.hstack(probabilities)\n",
    "        \n",
    "    def predict(self, X: ndarray):\n",
    "        print(self.w_)\n",
    "        return np.argmax(self.predict_probabilities(X), axis=1)\n",
    "        \n",
    "    def fit(self, data: ndarray, targets: ndarray) -> None:\n",
    "        num_samples = data.shape[0] # Rows\n",
    "        num_features = data.shape[1] # Columns\n",
    "        self.unique_target_classes = np.sort(np.unique(targets))\n",
    "        num_target_classes = len(self.unique_target_classes)\n",
    "        self.fit_classifiers = []\n",
    "        \n",
    "        for _, target in enumerate(self.unique_target_classes):\n",
    "            target_binary = np.array(targets == target).astype(int)\n",
    "            \n",
    "            classifier = self.solver(learn_rate=self.learn_rate, iterations=self.iterations, constant=self.constant, penalty=self.penalty)\n",
    "            classifier.fit(data, target_binary)\n",
    "            \n",
    "            self.fit_classifiers.append(classifier)\n",
    "        self.w_ = np.hstack([classifier.w_ for classifier in self.fit_classifiers]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class steepest_descent(binary_logistic_classifier):\n",
    "    def _get_gradient(self, X: ndarray, y: ndarray) -> ndarray:\n",
    "        ydiff = y - self.predict_probability(X, add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # compute gradient using all the training examples\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        return gradient\n",
    "        \n",
    "class stochastic_descent(binary_logistic_classifier):\n",
    "    def _get_gradient(self, X: ndarray, y: ndarray) -> ndarray:\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_probability(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        return gradient\n",
    "        \n",
    "class newton_method(binary_logistic_classifier):\n",
    "    \n",
    "    def _calculate_hessian(self, X: ndarray, y: ndarray) -> ndarray:\n",
    "            pred_weights = self.predict_probability(X, add_bias=False).ravel()\n",
    "            relu = self._relu(pred_weights)\n",
    "            grad = relu * (1 - relu)\n",
    "            hessian = grad @ X ** 2\n",
    "            return hessian\n",
    "    \n",
    "    def _get_gradient(self, X: ndarray, y: ndarray) -> ndarray: \n",
    "        pred_weights = self.predict_probability(X, add_bias=False).ravel()\n",
    "        ydiff = y - pred_weights\n",
    "        hessian = self._calculate_hessian(X, y)\n",
    "        \n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return pinv(hessian + np.outer(gradient, gradient)) @ gradient\n",
    "\n",
    "# The steps for this are detailed in the notebook. Implement them. That is, use BFGS without the use of an external package (for example, do not use SciPy).\n",
    "# I can only assume that this does not include the restriction of not using numpy...\n",
    "class bfgs(binary_logistic_classifier):\n",
    "\n",
    "    def _get_gradient(self, data: ndarray, target: ndarray) -> ndarray:\n",
    "        target_delta = target - self.predict_probability(data, add_bias=False).ravel()\n",
    "        gradient = np.mean(data * target_delta[:, np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        return -self._penalty(gradient)\n",
    "    \n",
    "    def fit(self, data: ndarray, target: ndarray) -> None:\n",
    "        data_bias = self._add_bias(data)\n",
    "        num_features = data_bias.shape[1] # Columns\n",
    "        self.w_ = np.zeros((num_features, 1)).flatten()\n",
    "        \n",
    "        # 1. Initial Approx. Hessian for k = 0 is an identity matrix\n",
    "        k = 0\n",
    "        H_k = np.identity(len(self.w_))\n",
    "        Hi_k = H_k # Inverse of I is I\n",
    "        \n",
    "        while(np.amax(np.abs(self._get_gradient(data_bias, target))) > 1e-03 and k < self.iterations):\n",
    "            # 2. Find update diretion p_k\n",
    "            gradient = self._get_gradient(data_bias, target)\n",
    "            p_k = -Hi_k @ gradient\n",
    "            # 3. Update w\n",
    "            new_weights = self.w_ + self.learn_rate * p_k\n",
    "            # 4. Save scaled direction\n",
    "            s_k = self.learn_rate * p_k\n",
    "            # 5a. Approximate change in derivative\n",
    "            self.w_ = new_weights\n",
    "            v_k = self._get_gradient(data_bias, target) - gradient\n",
    "            # 5b. Define u from above\n",
    "            #u_k = v_k - (H_k @ s_k)\n",
    "            # 6. Redefine approx. Hessian\n",
    "            #H_k = H_k + ((v_k @ v_k.T)/(v_k.T @ s_k)) - (((H_k @ s_k) @ (s_k.T @ H_k))/(s_k.T @ H_k @ s_k))\n",
    "            # 7. Approximate Inverse Hessian via Sherman Morris\n",
    "            numerator_one = (s_k[np.newaxis, :] @ v_k[:, np.newaxis] + v_k[np.newaxis, :] @ Hi_k @ v_k[:, np.newaxis]) * (s_k[:, np.newaxis] @ s_k[np.newaxis, :])\n",
    "            denominator_one = (s_k[np.newaxis, :] @ v_k[:, np.newaxis])**2\n",
    "            numerator_two = Hi_k @ v_k[:, np.newaxis] @ s_k[np.newaxis, :] + s_k[:, np.newaxis] @ v_k[np.newaxis, :] @ Hi_k\n",
    "            denominator_two = s_k[np.newaxis, :] @ v_k[:, np.newaxis]\n",
    "            Hi_k = Hi_k + numerator_one/denominator_one - numerator_two/denominator_two\n",
    "            # 8. Update k & start from step 2\n",
    "            k = k + 1\n",
    "            \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.20010249e-04 -3.41531352e-02 -3.50795102e-02 -1.50618760e-01\n",
      "  -6.06613525e-03 -1.27337602e-02 -1.17510249e-04 -1.82510249e-04\n",
      "  -1.35010249e-04]\n",
      " [-1.96614779e-02 -2.90556685e+00 -3.00229823e+00 -1.48794227e+01\n",
      "  -3.92567853e-01 -1.03972023e+00 -1.09902279e-02 -1.57502279e-02\n",
      "  -1.29389779e-02]\n",
      " [-1.60496094e-04 -2.40834961e-02 -2.48967461e-02 -1.08346746e-01\n",
      "  -4.63249609e-03 -1.67342461e-02 -8.17460937e-05 -1.56746094e-04\n",
      "  -8.29960937e-05]\n",
      " [-1.86479054e-04 -2.63486041e-02 -2.72153541e-02 -1.52821479e-01\n",
      "  -4.40647905e-03 -8.95397905e-03 -1.02729054e-04 -1.52729054e-04\n",
      "  -1.28979054e-04]\n",
      " [-4.05514515e-05 -6.03592645e-03 -6.23467645e-03 -2.99555515e-02\n",
      "  -8.73926452e-04 -2.52305145e-03 -2.43014515e-05 -3.43014515e-05\n",
      "  -2.30514515e-05]\n",
      " [-1.00335313e-04 -1.43198353e-02 -1.47905853e-02 -7.60278353e-02\n",
      "  -1.77208531e-03 -1.03890853e-02 -6.03353126e-05 -7.78353126e-05\n",
      "  -6.78353126e-05]]\n",
      "Accuracy: 0.01125\n",
      "CPU times: user 23.3 ms, sys: 8.75 ms, total: 32 ms\n",
      "Wall time: 28.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds = load_iris()\n",
    "X = df_train.drop(\"Failure Type\", axis=1)\n",
    "y = df_train[\"Failure Type\"]\n",
    "lr = multiclass_logistic_classifier(solver=steepest_descent, learn_rate=0.001, iterations=10, constant=0.1, penalty=\"L1\")\n",
    "lr.fit(X, y)\n",
    "y_hat = lr.predict(X)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-2.08599611e-02 -3.01065996e+00 -3.11325996e+00 -1.62498600e+01\n",
      "  -3.54259961e-01 -1.54485996e+00 -1.08599611e-02 -1.78599611e-02\n",
      "  -1.38599611e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "Accuracy: 0.01125\n",
      "CPU times: user 104 ms, sys: 7.44 ms, total: 111 ms\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds = load_iris()\n",
    "X = df_train.drop(\"Failure Type\", axis=1)\n",
    "y = df_train[\"Failure Type\"]\n",
    "lr = multiclass_logistic_classifier(solver=stochastic_descent, learn_rate=0.001, iterations=10, constant=0.1, penalty=\"L1\")\n",
    "lr.fit(X, y)\n",
    "y_hat = lr.predict(X)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.08999157e-10 -1.69204070e-08 -1.73793587e-08 -7.46206959e-08\n",
      "  -3.00533103e-09 -6.30865670e-09 -5.82178247e-11 -9.04206208e-11\n",
      "  -6.68878082e-11]\n",
      " [-1.02555523e-12 -1.51556221e-10 -1.56601791e-10 -7.76120183e-10\n",
      "  -2.04765897e-11 -5.42324705e-11 -5.73257301e-13 -8.21541938e-13\n",
      "  -6.74905344e-13]\n",
      " [-1.51850900e-10 -2.27862279e-08 -2.35556718e-08 -1.02510600e-07\n",
      "  -4.38296463e-09 -1.58328485e-08 -7.73428041e-11 -1.48302896e-10\n",
      "  -7.85254723e-11]\n",
      " [-9.38194072e-11 -1.32562364e-08 -1.36923066e-08 -7.68859573e-08\n",
      "  -2.21694203e-09 -4.50483306e-09 -5.16839759e-11 -7.68394572e-11\n",
      "  -6.48906036e-11]\n",
      " [-5.18350942e-10 -7.71545294e-08 -7.96950611e-08 -3.82908323e-07\n",
      "  -1.11710082e-08 -3.22510304e-08 -3.10634510e-10 -4.38460007e-10\n",
      "  -2.94656323e-10]\n",
      " [-1.98991259e-10 -2.83999918e-08 -2.93336126e-08 -1.50783152e-07\n",
      "  -3.51451028e-09 -2.06042829e-08 -1.19660760e-10 -1.54367854e-10\n",
      "  -1.34535229e-10]]\n",
      "Accuracy: 0.01125\n",
      "CPU times: user 1.82 s, sys: 811 ms, total: 2.63 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds = load_iris()\n",
    "X = df_train.drop(\"Failure Type\", axis=1)\n",
    "y = df_train[\"Failure Type\"]\n",
    "lr = multiclass_logistic_classifier(solver=newton_method, learn_rate=0.001, iterations=10, constant=0.1, penalty=\"L1\")\n",
    "lr.fit(X, y)\n",
    "y_hat = lr.predict(X)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A breakdown of our hyperparameters and the thinking behind their selection:\n",
    "\n",
    "- learn_rate: 0.001, A relatively high learning rate (especially compared to values like 1+e-0# which appear to be more typical in ML), which is used in conjuction with the iterations allowed to determine how far we need to be searching forward. Since we are only using 1 iteration, we need to tarvel across our function quickly to converge.\n",
    "- iterations: 1, Used to match Dr. Larson's iterations used for the sake of time comparisons.\n",
    "- constant: 0.1, Used to help emphasize the feature selection done by L1 in conjuction with RELU to hopefully sparsify our features quickly & find only the relevant terms. Still not too high at to not mistakenly wipe out relevant features.\n",
    "- penalty: L1, Since we have few features, outliers are a concern, prompting the usage of L1 to feature select and avoiding L2 since the sum of squares will explode if it runs into an outlier and penalize all of the other features that migth be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.83474783e-03 -2.43882211e-01 -2.70705452e-01  9.88711446e-02\n",
      "  -9.40740978e-02  5.69987093e-02  2.50082467e-03  3.18416632e-03\n",
      "   1.59384256e-03]\n",
      " [-8.23753196e-03 -7.44347617e+00 -7.65935155e+00  4.37849077e+00\n",
      "  -1.00367829e+00  2.37083918e+00  1.23089910e-02  4.39428936e-03\n",
      "   7.54526449e-03]\n",
      " [ 1.14998698e-04 -1.08009002e-01 -1.10705434e-01  4.77018382e-02\n",
      "  -5.46687150e-03  3.40444762e-02  1.78883506e-04  1.44105924e-03\n",
      "  -5.61015620e-04]\n",
      " [ 1.39967336e-03  3.03737301e-01  3.16883898e-01 -1.38354489e-01\n",
      "   2.71811257e-01  1.26868878e-01  2.07496960e-04  7.45922071e-04\n",
      "   1.12770772e-03]\n",
      " [ 2.44312595e-04 -3.93499476e-02 -4.02991835e-02  2.20659422e-02\n",
      "  -1.48571734e-02 -8.36015788e-02  4.89365351e-04  3.48416734e-04\n",
      "   1.59008533e-04]\n",
      " [ 7.11080634e-04  7.81549780e-02  8.10759863e-02 -2.52030580e-02\n",
      "   3.01892642e-02 -6.18899623e-02  7.11649333e-04  3.60179478e-04\n",
      "   5.32966906e-04]]\n",
      "Accuracy: 0.96525\n",
      "CPU times: user 3.76 s, sys: 1.77 s, total: 5.53 s\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds = load_iris()\n",
    "X = df_train.drop(\"Failure Type\", axis=1)\n",
    "y = df_train[\"Failure Type\"]\n",
    "lr = multiclass_logistic_classifier(solver=bfgs, learn_rate=0.001, iterations=10, constant=0.1, penalty=\"L1\")\n",
    "lr.fit(X, y)\n",
    "y_hat = lr.predict(X)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll start this section since I need to compare performance between BFGS implementations anyways\n",
    "# Feel free to make stuff above this since the BFGS implementation is the \"last\" classifer we make if following the instructions\n",
    "\n",
    "# Using the example BFGS class...\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing \n",
    "\n",
    "            \n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "\n",
    "        \n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=BFGSBinaryLogisticRegression):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-6.28665358e-04 -1.88570489e-01 -1.94883158e-01 -9.70222061e-01\n",
      "  -2.49557557e-02 -6.78786548e-02 -6.35096717e-05 -3.75269807e-04\n",
      "  -1.89885879e-04]\n",
      " [ 6.29084842e-04  1.88690111e-01  1.95008374e-01  9.70298550e-01\n",
      "   2.46879884e-02  6.61696857e-02  6.45648602e-05  3.71670491e-04\n",
      "   1.92849491e-04]\n",
      " [-6.29304697e-04 -1.88799883e-01 -1.95091699e-01 -9.70196701e-01\n",
      "  -2.49960353e-02 -6.69836972e-02 -6.42734980e-05 -3.73010127e-04\n",
      "  -1.92021073e-04]\n",
      " [-6.31788466e-04 -1.89545494e-01 -1.95864906e-01 -9.69795536e-01\n",
      "  -2.51644481e-02 -6.83537246e-02 -6.43041695e-05 -3.77947756e-04\n",
      "  -1.89536540e-04]\n",
      " [-6.30334744e-04 -1.89107242e-01 -1.95411207e-01 -9.69991111e-01\n",
      "  -2.52034458e-02 -6.80761523e-02 -6.32866209e-05 -3.77030044e-04\n",
      "  -1.90018079e-04]\n",
      " [-6.30644259e-04 -1.89199931e-01 -1.95507377e-01 -9.69993505e-01\n",
      "  -2.52455795e-02 -6.74902370e-02 -6.31917645e-05 -3.77877201e-04\n",
      "  -1.89575293e-04]]\n",
      "Accuracy of:  0.96525\n",
      "CPU times: user 13.5 s, sys: 5.92 s, total: 19.4 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = df_train.drop(\"Failure Type\", axis=1)\n",
    "y_not_binary = df_train[\"Failure Type\"]\n",
    "\n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "                                  iterations=10,\n",
    "                                  C=0.01,\n",
    "                                  solver=BFGSBinaryLogisticRegression\n",
    "                                 )\n",
    "lr.fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the immediate comparison, we end up with both implementations of BFGS getting the same 96.5% accuracy on our dataset, albeit through a vastly different set of weights. However, we should also consider the speed: both CPU and wall times see a significant slowdown in the scipy implementation, with a speed several (3~4x) times slower than our custom implemntation. Of course, the absolute magnitude of these values is small enough that CPU jitter is going to significantly affect the results, but there are other considerations to keep in mind.\n",
    "\n",
    "For instance, the scipy implementation computes the inverse hessian through the Sherman-Morrison algorithm just like ours, but where we use the expanded form to avoid computing matrix inverses, they instead utilize these temporary matrices, which may result in inefficiencies. We also do not use a line search algorithm to find our learning rate on each iteration (as it was not delineated as one of the steps in the instructions), and instead we tune an aggregate manually until good results are achieved. This avoids needing to run the line search, but also in effect means needing to run BFGS multiple times to optimize this hyperparameter, effectively making it more difficult to apply with good results and take longer overall.\n",
    "\n",
    "As such, we would still recommend using the scipy implementation of BFGS, although perhaps an improved version using the expanded Sherman-Morrison formula could be used to increase speed performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Others go here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965\n",
      "CPU times: user 56 ms, sys: 26.8 ms, total: 82.8 ms\n",
      "Wall time: 80.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ds = load_iris()\n",
    "X = df_test.drop(\"Failure Type\", axis=1)\n",
    "y = df_test[\"Failure Type\"]\n",
    "# We already have a model/agent fit on the training data\n",
    "#lr = multiclass_logistic_classifier(solver=bfgs, learn_rate=0.001, iterations=10, constant=0.1, penalty=\"L1\")\n",
    "#lr.fit(X, y) \n",
    "y_hat = lr.predict(X)\n",
    "print(\"Accuracy: \" + str(accuracy_score(y, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only ~2000 samples the time remains, as expected, fairly small (especially since we are only inferencing and not training), but achieving an even higher accuracy on the testing dataset than the training dataset is a great result! It appears that we were able to successfully meet our goal of creating an accurate & robust classifier, at least as far as the BFGS algorithm implementation is concerned.\n",
    "\n",
    "#### Potentially add on more to this section depending on how the other tests went? <b>@Wali</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Answer goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-ipython-kernel",
   "language": "python",
   "name": "school-ipython-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
